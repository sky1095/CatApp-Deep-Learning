{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CatApp.ipynb","version":"0.3.2","views":{},"default_view":{},"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"metadata":{"id":"MeYJ3BxHg-4D","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["import time\n","import numpy as np\n","import h5py\n","import matplotlib.pyplot as plt\n","import scipy\n","from PIL import Image\n","from scipy import ndimage\n","from test import *\n","\n","%matplotlib inline\n","plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n","plt.rcParams['image.interpolation'] = 'nearest'\n","plt.rcParams['image.cmap'] = 'gray'\n","\n","%load_ext autoreload\n","%autoreload 2\n","\n","np.random.seed(1)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"D4Hy76Ni4yci","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import h5py\n","\n","\n","def sigmoid(Z):\n","    \"\"\"\n","    Implements the sigmoid activation in numpy\n","    \n","    Arguments:\n","    Z -- numpy array of any shape\n","    \n","    Returns:\n","    A -- output of sigmoid(z), same shape as Z\n","    cache -- returns Z as well, useful during backpropagation\n","    \"\"\"\n","    \n","    A = 1/(1+np.exp(-Z))\n","    cache = Z\n","    \n","    return A, cache\n","\n","def relu(Z):\n","    \"\"\"\n","    Implement the RELU function.\n","\n","    Arguments:\n","    Z -- Output of the linear layer, of any shape\n","\n","    Returns:\n","    A -- Post-activation parameter, of the same shape as Z\n","    cache -- a python dictionary containing \"A\" ; stored for computing the backward pass efficiently\n","    \"\"\"\n","    \n","    A = np.maximum(0,Z)\n","    \n","    assert(A.shape == Z.shape)\n","    \n","    cache = Z \n","    return A, cache\n","\n","\n","def relu_backward(dA, cache):\n","    \"\"\"\n","    Implement the backward propagation for a single RELU unit.\n","\n","    Arguments:\n","    dA -- post-activation gradient, of any shape\n","    cache -- 'Z' where we store for computing backward propagation efficiently\n","\n","    Returns:\n","    dZ -- Gradient of the cost with respect to Z\n","    \"\"\"\n","    \n","    Z = cache\n","    dZ = np.array(dA, copy=True) # just converting dz to a correct object.\n","    \n","    # When z <= 0, you should set dz to 0 as well. \n","    dZ[Z <= 0] = 0\n","    \n","    assert (dZ.shape == Z.shape)\n","    \n","    return dZ\n","\n","def sigmoid_backward(dA, cache):\n","    \"\"\"\n","    Implement the backward propagation for a single SIGMOID unit.\n","\n","    Arguments:\n","    dA -- post-activation gradient, of any shape\n","    cache -- 'Z' where we store for computing backward propagation efficiently\n","\n","    Returns:\n","    dZ -- Gradient of the cost with respect to Z\n","    \"\"\"\n","    \n","    Z = cache\n","    \n","    s = 1/(1+np.exp(-Z))\n","    dZ = dA * s * (1-s)\n","    \n","    assert (dZ.shape == Z.shape)\n","    \n","    return dZ\n","\n","\n","def load_data():\n","    train_dataset = h5py.File('datasets/train_catvnoncat.h5', \"r\")\n","    train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:]) # your train set features\n","    train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:]) # your train set labels\n","\n","    test_dataset = h5py.File('datasets/test_catvnoncat.h5', \"r\")\n","    test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:]) # your test set features\n","    test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:]) # your test set labels\n","\n","    classes = np.array(test_dataset[\"list_classes\"][:]) # the list of classes\n","    \n","    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n","    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n","    \n","    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes\n","\n","\n","def initialize_parameters(n_x, n_h, n_y):\n","    \"\"\"\n","    Argument:\n","    n_x -- size of the input layer\n","    n_h -- size of the hidden layer\n","    n_y -- size of the output layer\n","    \n","    Returns:\n","    parameters -- python dictionary containing your parameters:\n","                    W1 -- weight matrix of shape (n_h, n_x)\n","                    b1 -- bias vector of shape (n_h, 1)\n","                    W2 -- weight matrix of shape (n_y, n_h)\n","                    b2 -- bias vector of shape (n_y, 1)\n","    \"\"\"\n","    \n","    np.random.seed(1)\n","    \n","    W1 = np.random.randn(n_h, n_x)*0.01\n","    b1 = np.zeros((n_h, 1))\n","    W2 = np.random.randn(n_y, n_h)*0.01\n","    b2 = np.zeros((n_y, 1))\n","    \n","    assert(W1.shape == (n_h, n_x))\n","    assert(b1.shape == (n_h, 1))\n","    assert(W2.shape == (n_y, n_h))\n","    assert(b2.shape == (n_y, 1))\n","    \n","    parameters = {\"W1\": W1,\n","                  \"b1\": b1,\n","                  \"W2\": W2,\n","                  \"b2\": b2}\n","    \n","    return parameters     \n","\n","\n","def initialize_parameters_deep(layer_dims):\n","    \"\"\"\n","    Arguments:\n","    layer_dims -- python array (list) containing the dimensions of each layer in our network\n","    \n","    Returns:\n","    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n","                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n","                    bl -- bias vector of shape (layer_dims[l], 1)\n","    \"\"\"\n","    \n","    np.random.seed(1)\n","    parameters = {}\n","    L = len(layer_dims)            # number of layers in the network\n","\n","    for l in range(1, L):\n","        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) / np.sqrt(layer_dims[l-1]) #*0.01\n","        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n","        \n","        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n","        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n","\n","        \n","    return parameters\n","\n","def linear_forward(A, W, b):\n","    \"\"\"\n","    Implement the linear part of a layer's forward propagation.\n","\n","    Arguments:\n","    A -- activations from previous layer (or input data): (size of previous layer, number of examples)\n","    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n","    b -- bias vector, numpy array of shape (size of the current layer, 1)\n","\n","    Returns:\n","    Z -- the input of the activation function, also called pre-activation parameter \n","    cache -- a python dictionary containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n","    \"\"\"\n","    \n","    Z = W.dot(A) + b\n","    \n","    assert(Z.shape == (W.shape[0], A.shape[1]))\n","    cache = (A, W, b)\n","    \n","    return Z, cache\n","\n","def linear_activation_forward(A_prev, W, b, activation):\n","    \"\"\"\n","    Implement the forward propagation for the LINEAR->ACTIVATION layer\n","\n","    Arguments:\n","    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n","    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n","    b -- bias vector, numpy array of shape (size of the current layer, 1)\n","    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n","\n","    Returns:\n","    A -- the output of the activation function, also called the post-activation value \n","    cache -- a python dictionary containing \"linear_cache\" and \"activation_cache\";\n","             stored for computing the backward pass efficiently\n","    \"\"\"\n","    \n","    if activation == \"sigmoid\":\n","        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n","        Z, linear_cache = linear_forward(A_prev, W, b)\n","        A, activation_cache = sigmoid(Z)\n","    \n","    elif activation == \"relu\":\n","        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n","        Z, linear_cache = linear_forward(A_prev, W, b)\n","        A, activation_cache = relu(Z)\n","    \n","    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n","    cache = (linear_cache, activation_cache)\n","\n","    return A, cache\n","\n","def L_model_forward(X, parameters):\n","    \"\"\"\n","    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation\n","    \n","    Arguments:\n","    X -- data, numpy array of shape (input size, number of examples)\n","    parameters -- output of initialize_parameters_deep()\n","    \n","    Returns:\n","    AL -- last post-activation value\n","    caches -- list of caches containing:\n","                every cache of linear_relu_forward() (there are L-1 of them, indexed from 0 to L-2)\n","                the cache of linear_sigmoid_forward() (there is one, indexed L-1)\n","    \"\"\"\n","\n","    caches = []\n","    A = X\n","    L = len(parameters) // 2                  # number of layers in the neural network\n","    \n","    # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n","    for l in range(1, L):\n","        A_prev = A \n","        A, cache = linear_activation_forward(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], activation = \"relu\")\n","        caches.append(cache)\n","    \n","    # Implement LINEAR -> SIGMOID. Add \"cache\" to the \"caches\" list.\n","    AL, cache = linear_activation_forward(A, parameters['W' + str(L)], parameters['b' + str(L)], activation = \"sigmoid\")\n","    caches.append(cache)\n","    \n","    assert(AL.shape == (1,X.shape[1]))\n","            \n","    return AL, caches\n","\n","def compute_cost(AL, Y):\n","    \"\"\"\n","    Implement the cost function defined by equation (7).\n","\n","    Arguments:\n","    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n","    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n","\n","    Returns:\n","    cost -- cross-entropy cost\n","    \"\"\"\n","    \n","    m = Y.shape[1]\n","\n","    # Compute loss from aL and y.\n","    cost = (1./m) * (-np.dot(Y,np.log(AL).T) - np.dot(1-Y, np.log(1-AL).T))\n","    \n","    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n","    assert(cost.shape == ())\n","    \n","    return cost\n","\n","def linear_backward(dZ, cache):\n","    \"\"\"\n","    Implement the linear portion of backward propagation for a single layer (layer l)\n","\n","    Arguments:\n","    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n","    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n","\n","    Returns:\n","    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n","    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n","    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n","    \"\"\"\n","    A_prev, W, b = cache\n","    m = A_prev.shape[1]\n","\n","    dW = 1./m * np.dot(dZ,A_prev.T)\n","    db = 1./m * np.sum(dZ, axis = 1, keepdims = True)\n","    dA_prev = np.dot(W.T,dZ)\n","    \n","    assert (dA_prev.shape == A_prev.shape)\n","    assert (dW.shape == W.shape)\n","    assert (db.shape == b.shape)\n","    \n","    return dA_prev, dW, db\n","\n","def linear_activation_backward(dA, cache, activation):\n","    \"\"\"\n","    Implement the backward propagation for the LINEAR->ACTIVATION layer.\n","    \n","    Arguments:\n","    dA -- post-activation gradient for current layer l \n","    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n","    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n","    \n","    Returns:\n","    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n","    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n","    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n","    \"\"\"\n","    linear_cache, activation_cache = cache\n","    \n","    if activation == \"relu\":\n","        dZ = relu_backward(dA, activation_cache)\n","        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n","        \n","    elif activation == \"sigmoid\":\n","        dZ = sigmoid_backward(dA, activation_cache)\n","        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n","    \n","    return dA_prev, dW, db\n","\n","def L_model_backward(AL, Y, caches):\n","    \"\"\"\n","    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n","    \n","    Arguments:\n","    AL -- probability vector, output of the forward propagation (L_model_forward())\n","    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n","    caches -- list of caches containing:\n","                every cache of linear_activation_forward() with \"relu\" (there are (L-1) or them, indexes from 0 to L-2)\n","                the cache of linear_activation_forward() with \"sigmoid\" (there is one, index L-1)\n","    \n","    Returns:\n","    grads -- A dictionary with the gradients\n","             grads[\"dA\" + str(l)] = ... \n","             grads[\"dW\" + str(l)] = ...\n","             grads[\"db\" + str(l)] = ... \n","    \"\"\"\n","    grads = {}\n","    L = len(caches) # the number of layers\n","    m = AL.shape[1]\n","    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n","    \n","    # Initializing the backpropagation\n","    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n","    \n","    # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"AL, Y, caches\". Outputs: \"grads[\"dAL\"], grads[\"dWL\"], grads[\"dbL\"]\n","    current_cache = caches[L-1]\n","    grads[\"dA\" + str(L)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, activation = \"sigmoid\")\n","    \n","    for l in reversed(range(L-1)):\n","        # lth layer: (RELU -> LINEAR) gradients.\n","        current_cache = caches[l]\n","        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l + 2)], current_cache, activation = \"relu\")\n","        grads[\"dA\" + str(l + 1)] = dA_prev_temp\n","        grads[\"dW\" + str(l + 1)] = dW_temp\n","        grads[\"db\" + str(l + 1)] = db_temp\n","\n","    return grads\n","\n","def update_parameters(parameters, grads, learning_rate):\n","    \"\"\"\n","    Update parameters using gradient descent\n","    \n","    Arguments:\n","    parameters -- python dictionary containing your parameters \n","    grads -- python dictionary containing your gradients, output of L_model_backward\n","    \n","    Returns:\n","    parameters -- python dictionary containing your updated parameters \n","                  parameters[\"W\" + str(l)] = ... \n","                  parameters[\"b\" + str(l)] = ...\n","    \"\"\"\n","    \n","    L = len(parameters) // 2 # number of layers in the neural network\n","\n","    # Update rule for each parameter. Use a for loop.\n","    for l in range(L):\n","        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\" + str(l+1)]\n","        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\" + str(l+1)]\n","        \n","    return parameters\n","\n","def predict(X, y, parameters):\n","    \"\"\"\n","    This function is used to predict the results of a  L-layer neural network.\n","    \n","    Arguments:\n","    X -- data set of examples you would like to label\n","    parameters -- parameters of the trained model\n","    \n","    Returns:\n","    p -- predictions for the given dataset X\n","    \"\"\"\n","    \n","    m = X.shape[1]\n","    n = len(parameters) // 2 # number of layers in the neural network\n","    p = np.zeros((1,m))\n","    \n","    # Forward propagation\n","    probas, caches = L_model_forward(X, parameters)\n","\n","    \n","    # convert probas to 0/1 predictions\n","    for i in range(0, probas.shape[1]):\n","        if probas[0,i] > 0.5:\n","            p[0,i] = 1\n","        else:\n","            p[0,i] = 0\n","    \n","    #print results\n","    #print (\"predictions: \" + str(p))\n","    #print (\"true labels: \" + str(y))\n","    print(\"Accuracy: \"  + str(np.sum((p == y)/m)))\n","        \n","    return p\n","\n","def print_mislabeled_images(classes, X, y, p):\n","    \"\"\"\n","    Plots images where predictions and truth were different.\n","    X -- dataset\n","    y -- true labels\n","    p -- predictions\n","    \"\"\"\n","    a = p + y\n","    mislabeled_indices = np.asarray(np.where(a == 1))\n","    plt.rcParams['figure.figsize'] = (40.0, 40.0) # set default size of plots\n","    num_images = len(mislabeled_indices[0])\n","    for i in range(num_images):\n","        index = mislabeled_indices[1][i]\n","        \n","        plt.subplot(2, num_images, i + 1)\n","        plt.imshow(X[:,index].reshape(64,64,3), interpolation='nearest')\n","        plt.axis('off')\n","        plt.title(\"Prediction: \" + classes[int(p[0,index])].decode(\"utf-8\") + \" \\n Class: \" + classes[y[0,index]].decode(\"utf-8\"))\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"iSZyZAL__IEb","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"Nd5paIg-5PkT","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["### CONSTANTS ###\n","layers_dims = [12288, 20, 7, 5, 1] #  5-layer model"],"execution_count":0,"outputs":[]},{"metadata":{"id":"H-5ZocNEzbOJ","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# GRADED FUNCTION: L_layer_model\n","\n","def L_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):#lr was 0.009\n","    \"\"\"\n","    Implements a L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID.\n","    \n","    Arguments:\n","    X -- data, numpy array of shape (number of examples, num_px * num_px * 3)\n","    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n","    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n","    learning_rate -- learning rate of the gradient descent update rule\n","    num_iterations -- number of iterations of the optimization loop\n","    print_cost -- if True, it prints the cost every 100 steps\n","    \n","    Returns:\n","    parameters -- parameters learnt by the model. They can then be used to predict.\n","    \"\"\"\n","\n","    np.random.seed(1)\n","    costs = []                         # keep track of cost\n","    \n","    # Parameters initialization.\n","    ### START CODE HERE ###\n","    parameters = initialize_parameters_deep(layers_dims)\n","    ### END CODE HERE ###\n","    \n","    # Loop (gradient descent)\n","    for i in range(0, num_iterations):\n","\n","        # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\n","        ### START CODE HERE ### (≈ 1 line of code)\n","        AL, caches = L_model_forward(X, parameters)\n","        ### END CODE HERE ###\n","        \n","        # Compute cost.\n","        ### START CODE HERE ### (≈ 1 line of code)\n","        cost = compute_cost(AL, Y)\n","        ### END CODE HERE ###\n","    \n","        # Backward propagation.\n","        ### START CODE HERE ### (≈ 1 line of code)\n","        grads = L_model_backward(AL, Y, caches)\n","        ### END CODE HERE ###\n"," \n","        # Update parameters.\n","        ### START CODE HERE ### (≈ 1 line of code)\n","        parameters = update_parameters(parameters, grads, learning_rate)\n","        ### END CODE HERE ###\n","                \n","        # Print the cost every 100 training example\n","        if print_cost and i % 100 == 0:\n","            print (\"Cost after iteration %i: %f\" %(i, cost))\n","        if print_cost and i % 100 == 0:\n","            costs.append(cost)\n","            \n","    # plot the cost\n","    plt.plot(np.squeeze(costs))\n","    plt.ylabel('cost')\n","    plt.xlabel('iterations (per tens)')\n","    plt.title(\"Learning rate =\" + str(learning_rate))\n","    plt.show()\n","    \n","    return parameters"],"execution_count":0,"outputs":[]},{"metadata":{"id":"zrS0fXzNzexs","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"output_extras":[{}],"base_uri":"https://localhost:8080/","height":648},"outputId":"7e7eddaf-4432-40b7-d006-8017b2e0bfe1","executionInfo":{"status":"error","timestamp":1518688782984,"user_tz":-330,"elapsed":1006,"user":{"displayName":"Sudhanshu Yadav","photoUrl":"//lh5.googleusercontent.com/-YTDAP5EElYc/AAAAAAAAAAI/AAAAAAAAO4Q/iHnCFKNlGco/s50-c-k-no/photo.jpg","userId":"101450359431996759905"}}},"cell_type":"code","source":["train_x_orig, train_y, test_x_orig, test_y, classes = load_data()"],"execution_count":0,"outputs":[{"output_type":"error","ename":"OSError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","\u001b[0;32m<ipython-input-50-9230e6c38a73>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_x_orig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_x_orig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-47-e64a53f98f8d>\u001b[0m in \u001b[0;36mload_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m     \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'datasets/train_catvnoncat.h5'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m     \u001b[0mtrain_set_x_orig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train_set_x\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# your train set features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0mtrain_set_y_orig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train_set_y\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# your train set labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, **kwds)\u001b[0m\n\u001b[1;32m    267\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mphil\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m                 \u001b[0mfapl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fapl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m                 \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muserblock_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswmr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mswmr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0mflags\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r+'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n","\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n","\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n","\u001b[0;31mOSError\u001b[0m: Unable to open file (unable to open file: name = 'datasets/train_catvnoncat.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"]}]},{"metadata":{"id":"LrMoFNTlzjfq","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}